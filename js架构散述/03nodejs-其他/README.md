## Chrome V8 垃圾回收机制


## Node 爬虫
自动获取网页内容，是搜索引擎的重要组成部分，搜索引擎的优化相当大一部分是对爬虫的优化

robots.txt是一个文本文件，它是一个协议，不是一个命令，它是爬虫要查看的第一个文件，它告诉爬虫在服务器上什么文件是可以被查看的。搜索机器人就会按照该文件中的内容来确定访问的范围。

爬虫抓取的是html内容，对于传统的爬虫而言是无法获取ajax（异步）请求的渲染后的内容的，在还没放回结构渲染时，爬虫已经获取好html文本了，google据说已经搞定这块了。 这就是为啥SPA的web站点的一个大缺点。

img
![image](/static/reptile.png)

